<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-seewo.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-seewo.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-seewo.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="大数据,">










<meta name="description" content="LSTM（Long Short-Term Memory）LSTM出现背景：由于RNN存在梯度消失的问题，很难处理长序列的数据。为了解决RNN存在问题，后续人们对RNN做了改进，得到了RNN的特例LSTM，它可以避免常规RNN的梯度消失，因此在工业界得到了广泛的应用。LSTM模型是RNN的变体，它能够学习长期依赖，允许信息长期存在。 举个例子来讲：比如人们读文章的时候，人们会根据已经阅读过的内容来对">
<meta name="keywords" content="大数据">
<meta property="og:type" content="article">
<meta property="og:title" content="Long Short-Term Memory">
<meta property="og:url" content="http://yoursite.com/2019/04/29/lstm/index.html">
<meta property="og:site_name" content="视睿团队技术博客">
<meta property="og:description" content="LSTM（Long Short-Term Memory）LSTM出现背景：由于RNN存在梯度消失的问题，很难处理长序列的数据。为了解决RNN存在问题，后续人们对RNN做了改进，得到了RNN的特例LSTM，它可以避免常规RNN的梯度消失，因此在工业界得到了广泛的应用。LSTM模型是RNN的变体，它能够学习长期依赖，允许信息长期存在。 举个例子来讲：比如人们读文章的时候，人们会根据已经阅读过的内容来对">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://store-g1.seewo.com/common-resource%2F620e7f6a46694dbdbe8f303df05c6057">
<meta property="og:image" content="https://store-g1.seewo.com/common-resource%2Fa5419e0ba05d4b36bb378556240f6662">
<meta property="og:image" content="https://store-g1.seewo.com/common-resource%2F625071d14334430c85dde94921f5bf2f">
<meta property="og:image" content="https://store-g1.seewo.com/common-resource%2F21637b247bbf4a13b08b557ffdc914b5">
<meta property="og:image" content="https://store-g1.seewo.com/common-resource%2F877a60b99593429ca31ba40e78fcc5d4">
<meta property="og:image" content="https://store-g1.seewo.com/common-resource%2F53b5d182344442ceafc7e5f752a7f47e">
<meta property="og:image" content="https://store-g1.seewo.com/common-resource%2Fd07bb6ca41614f94a0954a5201b14e96">
<meta property="og:image" content="https://store-g1.seewo.com/common-resource%2Fd7087b005e8046638612683e49775ea5">
<meta property="og:image" content="https://store-g1.seewo.com/common-resource%2Fcadc44620ebd406098272dca81184270">
<meta property="og:image" content="https://store-g1.seewo.com/common-resource%2F27b8458089754f3ba4e5b89769d7d2c1">
<meta property="og:image" content="https://store-g1.seewo.com/common-resource%2Fe868422f059a437dad7f9c5d008517ce">
<meta property="og:image" content="https://store-g1.seewo.com/common-resource%2F6fd544aa83024fb987e91e9b27c888cf">
<meta property="og:image" content="https://store-g1.seewo.com/common-resource%2F548e013e69d04917a7eb8e1b578573c0">
<meta property="og:image" content="https://store-g1.seewo.com/common-resource%2F4c09ee68dcb44af7bdcb6d228f864ebd">
<meta property="og:image" content="https://store-g1.seewo.com/common-resource%2Ff0da8c73ddcb4aed80665f03ae93b6ec">
<meta property="og:image" content="https://store-g1.seewo.com/common-resource%2F8f81a772acbb401b84d60ed85244bb90">
<meta property="og:image" content="https://store-g1.seewo.com/common-resource%2F4771a42fdd0a4fbc9f8f1613039f544d">
<meta property="og:image" content="https://store-g1.seewo.com/common-resource%2F3adbcb24254246edbc33934814a2bc16">
<meta property="og:updated_time" content="2019-06-10T03:32:34.017Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Long Short-Term Memory">
<meta name="twitter:description" content="LSTM（Long Short-Term Memory）LSTM出现背景：由于RNN存在梯度消失的问题，很难处理长序列的数据。为了解决RNN存在问题，后续人们对RNN做了改进，得到了RNN的特例LSTM，它可以避免常规RNN的梯度消失，因此在工业界得到了广泛的应用。LSTM模型是RNN的变体，它能够学习长期依赖，允许信息长期存在。 举个例子来讲：比如人们读文章的时候，人们会根据已经阅读过的内容来对">
<meta name="twitter:image" content="https://store-g1.seewo.com/common-resource%2F620e7f6a46694dbdbe8f303df05c6057">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/04/29/lstm/">





  <title>Long Short-Term Memory | 视睿团队技术博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">视睿团队技术博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">前端，后台，移动开发，Windows，C++，AI，大数据，QA</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/29/lstm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="视睿科技">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="视睿团队技术博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Long Short-Term Memory</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-29T18:51:04+08:00">
                2019-04-29
              </time>
            

            

            
          </span>

          <span> ｜ </span><span class="fa fa-user-o"> </span><span style="color:#222;"> ml_hhy</span> 

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/数据挖掘/" itemprop="url" rel="index">
                    <span itemprop="name">数据挖掘</span>
                  </a>
                </span>

                
                
              
            </span>
          


          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/29/lstm/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/29/lstm/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="LSTM（Long-Short-Term-Memory）"><a href="#LSTM（Long-Short-Term-Memory）" class="headerlink" title="LSTM（Long Short-Term Memory）"></a>LSTM（Long Short-Term Memory）</h1><p>LSTM出现背景：由于RNN存在梯度消失的问题，很难处理长序列的数据。为了解决RNN存在问题，后续人们对RNN做了改进，得到了RNN的特例LSTM，它可以避免常规RNN的梯度消失，因此在工业界得到了广泛的应用。<br>LSTM模型是RNN的变体，它能够学习长期依赖，允许信息长期存在。</p>
<p>举个例子来讲：比如人们读文章的时候，人们会根据已经阅读过的内容来对后面的内容进行理解，不会把之前的东西都丢掉从头进行思考，对内容的理解是贯穿的。<br>传统的神经网络即RNN做不到这一点，LSTM是具有循环的网络，解决了信息无法长期存在的问题，在工业界普遍使用有良好的效果。</p>
<p><em>带循环的递归神经网络如下</em><br><img src="https://store-g1.seewo.com/common-resource%2F620e7f6a46694dbdbe8f303df05c6057" alt="带循环的递归神经网络"></p>
<h1 id="RNN与LSTM之间联系"><a href="#RNN与LSTM之间联系" class="headerlink" title="RNN与LSTM之间联系"></a>RNN与LSTM之间联系</h1><p>RNN具有如下的结构，每个序列索引位置t都有一个隐藏状态h(t)。<br><img src="https://store-g1.seewo.com/common-resource%2Fa5419e0ba05d4b36bb378556240f6662" alt="在这里插入图片描述"><br>如果略去每层都有的o(t),L(t),y(t)，则RNN的模型可以简化成如下图的形式：<br><img src="https://store-g1.seewo.com/common-resource%2F625071d14334430c85dde94921f5bf2f" alt="在这里插入图片描述"><br>可以看出h(t)由x(t)和h(t−1)得到。<br>得到h(t)后一方面用于当前层的模型损失计算，另一方面用于计算下一层的h(t+1)。</p>
<p>为了避免RNN的梯度消失，LSTM将tanh激活函数转为更为复杂的结构<br>LSTM的结构如下图：<img src="https://store-g1.seewo.com/common-resource%2F21637b247bbf4a13b08b557ffdc914b5" alt="在这里插入图片描述"><br>在下图中，每一行都带有一个向量，该向量从一个节点输出到其他节点的输入。 粉红色圆圈表示点向运算，如向量加法、点乘，而黄色框是学习神经网络层。 线的合并表示连接，而线的交叉表示其内容正在复制，副本将转到不同的位置。<img src="https://store-g1.seewo.com/common-resource%2F877a60b99593429ca31ba40e78fcc5d4" alt></p>
<h1 id="LSTM模型结构"><a href="#LSTM模型结构" class="headerlink" title="LSTM模型结构"></a>LSTM模型结构</h1><h2 id="细胞状态"><a href="#细胞状态" class="headerlink" title="细胞状态"></a>细胞状态</h2><p>我们可以看出每个序列位置t时刻除了跟RNN一样的隐藏状态h(t)，还多了一个穿过图的顶部的长横线<br>长直线称之为细胞状态（Cell State），记为C(t)。如下图<br><img src="https://store-g1.seewo.com/common-resource%2F53b5d182344442ceafc7e5f752a7f47e" alt="在这里插入图片描述"></p>
<h2 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a>遗忘门</h2><p>遗忘门（forget gate）决定我们会从细胞状态中丢弃什么信息<br>在LSTM中即以一定的概率控制是否遗忘上一层的隐藏细胞状态。遗忘门子结构如下图所示：<br><img src="https://store-g1.seewo.com/common-resource%2Fd07bb6ca41614f94a0954a5201b14e96" alt="在这里插入图片描述"><br>图中输入的有上一序列的隐藏状态h(t−1)和本序列数据x(t)，通过一个激活函数，一般是sigmoid，得到遗忘门的输出f(t)。由于sigmoid的输出f(t)在[0,1]之间，因此这里的输出ft代表了遗忘上一层隐藏细胞状态的概率。用数学表达式即为：<br> <img src="https://store-g1.seewo.com/common-resource%2Fd7087b005e8046638612683e49775ea5" alt="在这里插入图片描述"><br>其中Wf,Uf,bf为线性关系的系数和偏倚，和RNN中的类似。σ为sigmoid激活函数</p>
<h2 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a>输入门</h2><p>输入门它的作用是处理哪部分应该被添加到细胞状态中，也就是为什么被称为部分存储。它的子结构如下图：<br><img src="https://store-g1.seewo.com/common-resource%2Fcadc44620ebd406098272dca81184270" alt="在这里插入图片描述"><br>从图中可以看到输入门由两部分组成，第一部分使用了sigmoid激活函数，输出为i(t),第二部分使用了tanh激活函数，输出为a(t), 两者的结果后面会相乘再去更新细胞状态。用数学表达式即为：<br><img src="https://store-g1.seewo.com/common-resource%2F27b8458089754f3ba4e5b89769d7d2c1" alt="在这里插入图片描述"><br>其中Wi,Ui,bi,Wa,Ua,ba,为线性关系的系数和偏倚，和RNN中的类似。σ为sigmoid激活函数。</p>
<h4 id="更新细胞状态"><a href="#更新细胞状态" class="headerlink" title="更新细胞状态"></a>更新细胞状态</h4><p>根据遗忘门得到的概率值，丢弃掉一部分之前的细胞状态加上根据输入门得到的新的一部分细胞状态相加，得到此时刻的细胞状态<br><img src="https://store-g1.seewo.com/common-resource%2Fe868422f059a437dad7f9c5d008517ce" alt="在这里插入图片描述"></p>
<h2 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h2><p>输出门（output gate）由 o(t) 控制，在这一时刻的输出 h(t)和 y(t) 就是由输出门控制的<br>h(t)的更新由两部分组成，第一部分是o(t), 它由上一序列的隐藏状态h(t−1)和本序列数据x(t)，以及激活函数sigmoid得到，第二部分由隐藏状态C(t)和tanh激活函数组成<br><img src="https://store-g1.seewo.com/common-resource%2F6fd544aa83024fb987e91e9b27c888cf" alt="在这里插入图片描述"><br>简要来说，LSTM 单元能够学习到识别重要输入（输入门作用），存储进长时状态，并保存必要的时间（遗忘门功能），并学会提取当前输出所需要的记忆。</p>
<p>这也解释了 LSTM 单元能够在提取长时序列，长文本，录音等数据中的长期模式的惊人成功的原因。</p>
<h2 id="前向传播算法"><a href="#前向传播算法" class="headerlink" title="前向传播算法"></a>前向传播算法</h2><p>一个时刻的前向传播过程如下，对比RNN多了12个参数<br><img src="https://store-g1.seewo.com/common-resource%2F548e013e69d04917a7eb8e1b578573c0" alt="在这里插入图片描述"><br><img src="https://store-g1.seewo.com/common-resource%2F4c09ee68dcb44af7bdcb6d228f864ebd" alt="在这里插入图片描述"></p>
<h2 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h2><p>反向传播通过梯度下降法迭代更新所有的参数<br><img src="https://store-g1.seewo.com/common-resource%2Ff0da8c73ddcb4aed80665f03ae93b6ec" alt="在这里插入图片描述"><br>如图上的5个不同的阶段反向传播误差</p>
<p>先介绍下各个变量的维度，htht 的维度是黄框里隐藏层神经元的个数，记为d，即矩阵W∗W∗ 的行数（因为WjiWji是输入层的第ii个神经元指向隐藏层第jj个神经元的参数），xtxt的维度记为n，则[ht−1xt][ht−1xt]的维度是d+nd+n，矩阵的维度都是d∗(d+n)d∗(d+n)，其他的向量维度都是d∗1d∗1。（为了表示、更新方便，我们将bias放到矩阵里）<br>以Wf举例：<br><img src="https://store-g1.seewo.com/common-resource%2F8f81a772acbb401b84d60ed85244bb90" alt="在这里插入图片描述"><br>同理：<br><img src="https://store-g1.seewo.com/common-resource%2F4771a42fdd0a4fbc9f8f1613039f544d" alt="在这里插入图片描述"><br>合并为一个矩阵就是：<br><img src="https://store-g1.seewo.com/common-resource%2F3adbcb24254246edbc33934814a2bc16" alt="在这里插入图片描述"><br>⊙是element-wise乘，即按元素乘。其他的为正常的矩阵乘<br>用δztδzt表示EtEt对ztzt的偏导<br>⊗ 表示外积，即列向量乘以行向量</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>LSTM对比RNN简单来讲就是复杂了每层的计算方式，解决梯度消失，可以用于处理处理长序列的数据，所以在工业界得到了广泛的应用</p>
<h2 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h2><h2 id="使用keras实现LSTM-情感分析"><a href="#使用keras实现LSTM-情感分析" class="headerlink" title="使用keras实现LSTM 情感分析"></a>使用keras实现LSTM 情感分析</h2><p>keras提供一个LSTM层，用它来构造和训练一个多对一的RNN。我们的网络吸收一个序列（词序列）并输出一个情感分析值（正或负）。<br>训练集源自于kaggle上情感分类竞赛，包含7000个短句 UMICH SI650<br>每个句子有一个值为1或0的分别用来代替正负情感的标签，这个标签就是我们将要学习预测的。</p>
<h3 id="导入所需库"><a href="#导入所需库" class="headerlink" title="导入所需库"></a>导入所需库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Activation, Dense, Dropout, SpatialDropout1D</span><br><span class="line"><span class="keyword">from</span> keras.layers.embeddings <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="keyword">from</span> keras.layers.recurrent <span class="keyword">import</span> LSTM</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> sequence</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></table></figure>
<h3 id="探索性分析"><a href="#探索性分析" class="headerlink" title="探索性分析"></a>探索性分析</h3><p>特别地想知道语料中有多少个独立的词以及每个句子包含多少个词：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Read training data and generate vocabulary</span></span><br><span class="line">maxlen = <span class="number">0</span></span><br><span class="line">word_freqs = collections.Counter()</span><br><span class="line">num_recs = <span class="number">0</span></span><br><span class="line">ftrain = open(os.path.join(DATA_DIR, <span class="string">"umich-sentiment-train.txt"</span>), <span class="string">'rb'</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> ftrain:</span><br><span class="line">    label, sentence = line.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">    words = nltk.word_tokenize(sentence.decode(<span class="string">"ascii"</span>, <span class="string">"ignore"</span>).lower())</span><br><span class="line">    <span class="keyword">if</span> len(words) &gt; maxlen:</span><br><span class="line">        maxlen = len(words)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        word_freqs[word] += <span class="number">1</span></span><br><span class="line">    num_recs += <span class="number">1</span></span><br><span class="line">ftrain.close()</span><br></pre></td></tr></table></figure></p>
<p>通过上述代码，我们可以得到语料的值<br>maxlen: 42<br>len(word_freqs): 2313<br>我们将单词总数量设为固定值，并把所有其他词看作字典外的词，这些词全部用伪词unk（unknown）替换，预测时候将未见的词进行替换<br>句子包含的单词数（maxlen）让我们可以设置一个固定的序列长度，并且用0进行补足短句，把更长的句子截短至合适的长度。<br>把VOCABULARY_SIZE设置为2002，即源于字典的2000个词，加上伪词UNK和填充词PAD（用来补足句子到固定长度的词）<br>这里把句子最大长度MAX_SENTENCE_LENGTH定为40<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MAX_FEATURES = <span class="number">2000</span></span><br><span class="line">MAX_SENTENCE_LENGTH = <span class="number">40</span></span><br></pre></td></tr></table></figure></p>
<p>下一步我们需要两个查询表，RNN的每一个输入行都是一个词序列索引，索引按训练集中词的使用频度从高到低排序。这两张查询表允许我们通过给定的词来查找索引以及通过给定的索引来查找词。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1 is UNK, 0 is PAD</span></span><br><span class="line"><span class="comment"># We take MAX_FEATURES-1 featurs to accound for PAD</span></span><br><span class="line">vocab_size = min(MAX_FEATURES, len(word_freqs)) + <span class="number">2</span></span><br><span class="line">word2index = &#123;x[<span class="number">0</span>]: i+<span class="number">2</span> <span class="keyword">for</span> i, x <span class="keyword">in</span> </span><br><span class="line">                enumerate(word_freqs.most_common(MAX_FEATURES))&#125;</span><br><span class="line">word2index[<span class="string">"PAD"</span>] = <span class="number">0</span></span><br><span class="line">word2index[<span class="string">"UNK"</span>] = <span class="number">1</span></span><br><span class="line">index2word = &#123;v:k <span class="keyword">for</span> k, v <span class="keyword">in</span> word2index.items()&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="接着我们将序列转换成词索引序列"><a href="#接着我们将序列转换成词索引序列" class="headerlink" title="接着我们将序列转换成词索引序列"></a>接着我们将序列转换成词索引序列</h4><h4 id="补足MAX-SENTENCE-LENGTH定义的词的长度"><a href="#补足MAX-SENTENCE-LENGTH定义的词的长度" class="headerlink" title="补足MAX_SENTENCE_LENGTH定义的词的长度"></a>补足MAX_SENTENCE_LENGTH定义的词的长度</h4><h4 id="因为我们的输出标签是二分类（正负情感）"><a href="#因为我们的输出标签是二分类（正负情感）" class="headerlink" title="因为我们的输出标签是二分类（正负情感）"></a>因为我们的输出标签是二分类（正负情感）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># convert sentences to sequences</span></span><br><span class="line">X = np.empty((num_recs, ), dtype=list)</span><br><span class="line">y = np.zeros((num_recs, ))</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line">ftrain = open(os.path.join(DATA_DIR, <span class="string">"umich-sentiment-train.txt"</span>), <span class="string">'rb'</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> ftrain:</span><br><span class="line">    label, sentence = line.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">    words = nltk.word_tokenize(sentence.decode(<span class="string">"ascii"</span>, <span class="string">"ignore"</span>).lower())</span><br><span class="line">    seqs = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">if</span> word2index.has_key(word):</span><br><span class="line">            seqs.append(word2index[word])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            seqs.append(word2index[<span class="string">"UNK"</span>])</span><br><span class="line">    X[i] = seqs</span><br><span class="line">    y[i] = int(label)</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line">ftrain.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pad the sequences (left padded with zeros)</span></span><br><span class="line">X = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)</span><br></pre></td></tr></table></figure>
<h3 id="划分测试集与训练集"><a href="#划分测试集与训练集" class="headerlink" title="划分测试集与训练集"></a>划分测试集与训练集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Split input into training and test</span></span><br><span class="line">Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=<span class="number">0.2</span>, </span><br><span class="line">                                                random_state=<span class="number">42</span>)</span><br><span class="line">print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)</span><br></pre></td></tr></table></figure>
<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">EMBEDDING_SIZE = <span class="number">128</span></span><br><span class="line">HIDDEN_LAYER_SIZE = <span class="number">64</span></span><br><span class="line"><span class="comment"># 美伦批大小32</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line"><span class="comment"># 网络训练10轮</span></span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Build model</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Embedding(vocab_size, EMBEDDING_SIZE, </span><br><span class="line">                    input_length=MAX_SENTENCE_LENGTH))</span><br><span class="line">model.add(SpatialDropout1D(Dropout(<span class="number">0.2</span>)))</span><br><span class="line">model.add(LSTM(HIDDEN_LAYER_SIZE, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.2</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>))</span><br><span class="line">model.add(Activation(<span class="string">"sigmoid"</span>))</span><br><span class="line"></span><br><span class="line">model.compile(loss=<span class="string">"binary_crossentropy"</span>, optimizer=<span class="string">"adam"</span>, </span><br><span class="line">              metrics=[<span class="string">"accuracy"</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, </span><br><span class="line">                    epochs=NUM_EPOCHS,</span><br><span class="line">                    validation_data=(Xtest, ytest))</span><br></pre></td></tr></table></figure>
<h3 id="最后我们在测试集上评估模型并打印出评分和准确率"><a href="#最后我们在测试集上评估模型并打印出评分和准确率" class="headerlink" title="最后我们在测试集上评估模型并打印出评分和准确率"></a>最后我们在测试集上评估模型并打印出评分和准确率</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># evaluate</span></span><br><span class="line">score, acc = model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)</span><br><span class="line">print(<span class="string">"Test score: %.3f, accuracy: %.3f"</span> % (score, acc))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    idx = np.random.randint(len(Xtest))</span><br><span class="line">    xtest = Xtest[idx].reshape(<span class="number">1</span>,<span class="number">40</span>)</span><br><span class="line">    ylabel = ytest[idx]</span><br><span class="line">    ypred = model.predict(xtest)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    sent = <span class="string">" "</span>.join([index2word[x] <span class="keyword">for</span> x <span class="keyword">in</span> xtest[<span class="number">0</span>].tolist() <span class="keyword">if</span> x != <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"%.0f\t%d\t%s"</span> % (ypred, ylabel, sent))</span><br></pre></td></tr></table></figure>
<p>至此我们使用keras实现lstm的情感分析实例<br>在此实例中可学习到keras框架的使用、lstm模型搭建、短语句处理方式</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/大数据/" rel="tag"># 大数据</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/03/29/利用Xcode在非越狱机器上动态调试第三方APP/" rel="next" title="利用Xcode在非越狱机器上动态调试第三方APP">
                <i class="fa fa-chevron-left"></i> 利用Xcode在非越狱机器上动态调试第三方APP
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/05/29/前端国际化方案选择/" rel="prev" title="前端国际化方案选择">
                前端国际化方案选择 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">视睿科技</p>
              <p class="site-description motion-element" itemprop="description">视睿团队技术博客，包含各大技术栈知识共享、应用研究、质量与架构设计、前沿技术学习等方面。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#LSTM（Long-Short-Term-Memory）"><span class="nav-number">1.</span> <span class="nav-text">LSTM（Long Short-Term Memory）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RNN与LSTM之间联系"><span class="nav-number">2.</span> <span class="nav-text">RNN与LSTM之间联系</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LSTM模型结构"><span class="nav-number">3.</span> <span class="nav-text">LSTM模型结构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#细胞状态"><span class="nav-number">3.1.</span> <span class="nav-text">细胞状态</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#遗忘门"><span class="nav-number">3.2.</span> <span class="nav-text">遗忘门</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#输入门"><span class="nav-number">3.3.</span> <span class="nav-text">输入门</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#更新细胞状态"><span class="nav-number">3.3.0.1.</span> <span class="nav-text">更新细胞状态</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#输出门"><span class="nav-number">3.4.</span> <span class="nav-text">输出门</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#前向传播算法"><span class="nav-number">3.5.</span> <span class="nav-text">前向传播算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播算法"><span class="nav-number">3.6.</span> <span class="nav-text">反向传播算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#结论"><span class="nav-number">3.7.</span> <span class="nav-text">结论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#代码实例"><span class="nav-number">3.8.</span> <span class="nav-text">代码实例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用keras实现LSTM-情感分析"><span class="nav-number">3.9.</span> <span class="nav-text">使用keras实现LSTM 情感分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#导入所需库"><span class="nav-number">3.9.1.</span> <span class="nav-text">导入所需库</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#探索性分析"><span class="nav-number">3.9.2.</span> <span class="nav-text">探索性分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#接着我们将序列转换成词索引序列"><span class="nav-number">3.9.2.1.</span> <span class="nav-text">接着我们将序列转换成词索引序列</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#补足MAX-SENTENCE-LENGTH定义的词的长度"><span class="nav-number">3.9.2.2.</span> <span class="nav-text">补足MAX_SENTENCE_LENGTH定义的词的长度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#因为我们的输出标签是二分类（正负情感）"><span class="nav-number">3.9.2.3.</span> <span class="nav-text">因为我们的输出标签是二分类（正负情感）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#划分测试集与训练集"><span class="nav-number">3.9.3.</span> <span class="nav-text">划分测试集与训练集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练模型"><span class="nav-number">3.9.4.</span> <span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#最后我们在测试集上评估模型并打印出评分和准确率"><span class="nav-number">3.9.5.</span> <span class="nav-text">最后我们在测试集上评估模型并打印出评分和准确率</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">视睿</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'l8zFpUKkKnP050Dd0dUFanlr-gzGzoHsz',
        appKey: 'V72yK9it7uzFFuexBxGyztxx',
        placeholder: '欢迎交流',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  





  

  

  

  
  

  

  

  

</body>
</html>
